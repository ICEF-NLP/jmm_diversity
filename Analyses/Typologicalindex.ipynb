{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "seasonal-career",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "#from nltk.util import ngrams\n",
    "\n",
    "import pandas as pd\n",
    "#import seaborn as sb\n",
    "\n",
    "import sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [15, 6]\n",
    "from pandas import DataFrame\n",
    "import lang2vec.lang2vec as l2v\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-township",
   "metadata": {},
   "source": [
    "# 1. Typological index using syntactic features from lang2vec (TI_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "correct-buffalo",
   "metadata": {},
   "outputs": [],
   "source": [
    "xcopa_codes=pd.read_csv('../Data/isomappings/xcopa-processed.10000.csv', index_col=0)\n",
    "xquad_codes=pd.read_csv('../Data/isomappings/xquad-processed.10000.csv', index_col=0)\n",
    "tydiqa_codes=pd.read_csv('../Data/isomappings/tydiqa-processed.10000.csv', index_col=0)\n",
    "xnli_codes=pd.read_csv('../Data/isomappings/xnli-processed.10000.csv', index_col=0)\n",
    "xtreme_codes=pd.read_csv('../Data/isomappings/xtreme-processed.10000.csv', index_col=0)\n",
    "xglue_codes=pd.read_csv('../Data/isomappings/xglue-processed.10000.csv', index_col=0)\n",
    "ud_codes=pd.read_csv('../Data/isomappings/ud-processed.tsv', sep='\\t', index_col=0)\n",
    "teddi_codes=pd.read_csv('../Data/isomappings/teddi500.csv', index_col=0)\n",
    "mbert_codes=pd.read_csv('../Data/isomappings/mbertwiki-processed.10000.csv', index_col=0)\n",
    "bible_codes=pd.read_csv('../Data/isomappings/biblecorpus100-processed.10000.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "institutional-palestinian",
   "metadata": {},
   "outputs": [],
   "source": [
    "#manual substitutions for problematic cases\n",
    "ud_codes.loc[\"UD_Western_Armenian-ArmTDP.txt\"].at[\"ISO_6393\"]=\"hy\"\n",
    "mbert_codes.loc[\"armenian\"].at[\"ISO_6393\"]=\"hy\"\n",
    "mbert_codes.loc[\"vowiki-latest-pages-articles\"].at[\"ISO_6393\"]=\"vol\"\n",
    "bible_codes=bible_codes.drop([\"crp.txt\"])\n",
    "bible_codes.loc[\"jap.txt\"].at[\"ISO_6393\"]=\"jpn\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-detection",
   "metadata": {},
   "source": [
    "- Function for extracting the syntactic feature vectors according to the ISO language codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "integrated-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_l2v(dataset_codes):\n",
    "    #list of iso codes to query the l2v vectors:\n",
    "    codes=dataset_codes[\"ISO_6393\"].str.lower().tolist()\n",
    "    #codes=xcopa_codes.index.tolist()\n",
    "    \n",
    "    features = l2v.get_features(codes, \"syntax_knn\")\n",
    "    #features = l2v.get_features(codes, \"syntax_average\")\n",
    "\n",
    "    features_frame = pd.DataFrame.from_dict(features).transpose()\n",
    "    return (features_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-cartoon",
   "metadata": {},
   "source": [
    "- Function for calculating the entropy (TI measure):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "preceding-hamilton",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy(df): #frame with features\n",
    "    entropies=[]\n",
    "    for index in range(len(df.columns)): #We are processing column by column\n",
    "        p= np.ones(2)\n",
    "        freqs=df[index].to_numpy() #We convert the column to a Numpy array\n",
    "        #We extract the number of zeros, and the number of ones:\n",
    "        ones=len(freqs[freqs == 1])\n",
    "        zeros=len(freqs[freqs == 0])\n",
    "        #We calculate the prob:\n",
    "        p_ones = ones/len(freqs)    #Probability (relative frequency), e.g., 8/11, 3/11\n",
    "        p_zeros = zeros/len(freqs) \n",
    "        p[0]=p_ones\n",
    "        p[1]=p_zeros\n",
    "        p=p[p != 0]  #We extract only the values not equal to zero. (otherwise we get a nan when aplying the algorithm)\n",
    "        H=-(p*np.log2(p)).sum()  #Entropy calculation\n",
    "        entropies.append(H)      #We store the entropy of each row in an array\n",
    "        #print(ones,zeros, p_ones, p_zeros, H)\n",
    "    return(entropies) #entropy feature-wise\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "federal-upper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xcopa 0.5862770789920277\n",
      "xquad 0.5231360114362537\n",
      "tydiqa 0.626102007988479\n",
      "xnli 0.5570648270071721\n",
      "xtreme 0.6116017688923218\n",
      "xglue 0.5167326017164338\n",
      "ud 0.5674507124838225\n",
      "teddi 0.7063106049093542\n",
      "mbert 0.5590294778394355\n",
      "bibles 0.649122540960284\n"
     ]
    }
   ],
   "source": [
    "entropies_features=get_entropy(get_l2v(xcopa_codes)) #entropies feature-wise\n",
    "print(\"xcopa\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_l2v(xquad_codes)) #entropies feature-wise\n",
    "print(\"xquad\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_l2v(tydiqa_codes)) #entropies feature-wise\n",
    "print(\"tydiqa\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_l2v(xnli_codes)) #entropies feature-wise\n",
    "print(\"xnli\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_l2v(xtreme_codes)) #entropies feature-wise\n",
    "print(\"xtreme\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_l2v(xglue_codes)) #entropies feature-wise\n",
    "print(\"xglue\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_l2v(ud_codes)) #entropies feature-wise\n",
    "print(\"ud\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_l2v(teddi_codes)) #entropies feature-wise\n",
    "print(\"teddi\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_l2v(mbert_codes)) #entropies feature-wise\n",
    "print(\"mbert\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_l2v(bible_codes)) #entropies feature-wise\n",
    "print(\"bibles\", mean(entropies_features)) #mean of entropies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valued-religion",
   "metadata": {},
   "source": [
    "# 2. Typological index using word length as features (TI_morph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "limited-milwaukee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#***All the code to do the sampling, calculating measures, and finally obtain these tsvs, is not included in this notebook***\n",
    "teddi_10k=pd.read_csv('../Data/wordlength_results/RESULTS_teddi_tokens/sample10000.tsv', sep='\\t',index_col=0)\n",
    "ud_10k=pd.read_csv('../Data/wordlength_results/RESULTS_ud-processed_tokens/ud-processed.10000.stats.tsv', sep='\\t',index_col=0)\n",
    "bibles_10k=pd.read_csv('../Data/wordlength_results/RESULTS_biblecorpus100-processed_tokens/biblecorpus100-processed.10000.stats.tsv', sep='\\t',index_col=0)\n",
    "xcopa_10k=pd.read_csv('../Data/wordlength_results/RESULTS_xcopa-processed_tokens/xcopa-processed.10000.stats.tsv', sep='\\t',index_col=0)\n",
    "tydiqa_10k=pd.read_csv('../Data/wordlength_results/RESULTS_tydiqa-processed_tokens/tydiqa-processed.10000.stats.tsv', sep='\\t',index_col=0)\n",
    "xquad_10k=pd.read_csv('../Data/wordlength_results/RESULTS_xquad-processed_tokens/xquad-processed.10000.stats.tsv', sep='\\t',index_col=0)\n",
    "xnli_10k=pd.read_csv('../Data/wordlength_results/RESULTS_xnli-processed_tokens/xnli-processed.10000.stats.tsv', sep='\\t',index_col=0)\n",
    "xglue_10k=pd.read_csv('../Data/wordlength_results/RESULTS_xglue-processed_tokens/xglue-processed.10000.stats.tsv', sep='\\t',index_col=0)\n",
    "xtreme_10k=pd.read_csv('../Data/wordlength_results/RESULTS_xtreme-processed_tokens/xtreme-processed.10000.stats.tsv', sep='\\t',index_col=0)\n",
    "mbert_10k=pd.read_csv('../Data/wordlength_results/RESULTS_mbertwiki-processed_tokens/mbertwiki-processed.10000.stats.tsv', sep='\\t',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-tackle",
   "metadata": {},
   "source": [
    "- Function for creating binary vectors, using the word lengths as features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sophisticated-cylinder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordlength_vectors(dataset):\n",
    "    #For each language initialize an array of elements (the maximum possoble word length in all lamguages)\n",
    "    langs=dataset.index.tolist()\n",
    "    vectors_hash={}\n",
    "    for l in langs:    \n",
    "\n",
    "        binary_vector= np.zeros(11)\n",
    "\n",
    "        #wordlength=dataset.loc[l]['Avg_length']\n",
    "        wordlength=dataset.loc[l]['Median_length'] #for median\n",
    "\n",
    "        #if Avg_length it is betwwen 0 and 1: assign 1 to the first bin (element of the array), and so on:\n",
    "        \n",
    "        #binary_vector[(round(wordlength))-1]=1 #binarization using round numbers\n",
    "        binary_vector[(int(wordlength))-1]=1  #binarization using the integer part of the number\n",
    "        #print (l, wordlength, round(wordlength), binary_vector)\n",
    "        #print (l, wordlength, binary_vector)\n",
    "        \n",
    "        vectors_hash[l]= binary_vector\n",
    "        \n",
    "    \n",
    "    return(pd.DataFrame.from_dict(vectors_hash).transpose()) #a dataframe with one vector per row "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-better",
   "metadata": {},
   "source": [
    "The  binarization is based on splitting the word lenght into bins . Example:\n",
    "\n",
    "If a language has word_length 4.9, its vector will have  \"1\" in the position 4. If  bins are splitted into: 0-1, 1-2, ...\n",
    "\n",
    "0 0 0 1 0 0 0 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "consolidated-gossip",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordlength_vectors(dataset, binsize):\n",
    "    #For each language initialize an array of elements (the maximum possoble word length in all lamguages)\n",
    "    \n",
    "    bins=np.arange(1, 11.2, binsize) #[ 1. ,  1.1,  1.2,  1.3... ]\n",
    "    \n",
    "    langs=dataset.index.tolist()\n",
    "    vectors_hash={}\n",
    "    \n",
    "    for l in langs:    \n",
    "\n",
    "        binary_vector= np.zeros(len(bins))\n",
    "\n",
    "        wordlength=dataset.loc[l]['Avg_length']\n",
    "        index=len(np.arange(1, wordlength, binsize)) #we partition the word length in the same bins, the total size of the array is the index for putting a 1 in the binary vector\n",
    "        #print(l, wordlength, index)\n",
    "        #index=bins.round(decimals=2).tolist().index(round(wordlength,1)) #locate the index that has that word length (index starts at zero)\n",
    "       \n",
    "\n",
    "        #if Avg_length it is betwwen 0 and 1: assign 1 to the first bin (element of the array), and so on:\n",
    "        \n",
    "        binary_vector[index-1]=1  \n",
    "        \n",
    "        vectors_hash[l]= binary_vector\n",
    "        \n",
    "    \n",
    "    return(pd.DataFrame.from_dict(vectors_hash).transpose()) #a dataframe with one vector per row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "occupied-burst",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bin Size: 1\n",
      "Vector features: 11\n"
     ]
    }
   ],
   "source": [
    "print(\"Bin Size: 1\")\n",
    "\n",
    "print(\"Vector features:\", str(len(np.arange(1, 11.2, 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "surprising-flooring",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xcopa 0.36103779596241575\n",
      "xquad 0.34093043571111287\n",
      "tydiqa 0.3433145652165989\n",
      "xnli 0.33874266714204293\n",
      "xtreme 0.31093208189005345\n",
      "xglue 0.3072073812912939\n",
      "ud 0.34890225383819307\n",
      "teddi 0.36874805539335226\n",
      "mbert 0.32349322087652765\n",
      "bibles 0.31080272059778274\n"
     ]
    }
   ],
   "source": [
    "entropies_features=get_entropy(get_wordlength_vectors(xcopa_10k, 1)) #entropies feature-wise\n",
    "print(\"xcopa\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(xquad_10k, 1)) #entropies feature-wise\n",
    "print(\"xquad\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(tydiqa_10k, 1)) #entropies feature-wise\n",
    "print(\"tydiqa\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(xnli_10k, 1)) #entropies feature-wise\n",
    "print(\"xnli\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(xtreme_10k, 1)) #entropies feature-wise\n",
    "print(\"xtreme\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(xglue_10k, 1)) #entropies feature-wise\n",
    "print(\"xglue\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(ud_10k, 1)) #entropies feature-wise\n",
    "print(\"ud\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(teddi_10k, 1)) #entropies feature-wise\n",
    "print(\"teddi\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(mbert_10k, 1)) #entropies feature-wise\n",
    "print(\"mbert\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(bibles_10k, 1)) #entropies feature-wise\n",
    "print(\"bibles\", mean(entropies_features)) #mean of entropies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
