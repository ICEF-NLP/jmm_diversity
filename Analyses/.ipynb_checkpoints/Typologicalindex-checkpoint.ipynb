{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "compound-maria",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "#from nltk.util import ngrams\n",
    "\n",
    "import pandas as pd\n",
    "#import seaborn as sb\n",
    "\n",
    "import sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [15, 6]\n",
    "from pandas import DataFrame\n",
    "import lang2vec.lang2vec as l2v\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-heater",
   "metadata": {},
   "source": [
    "# 1. Typological index using syntactic features (lang2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "superb-reward",
   "metadata": {},
   "outputs": [],
   "source": [
    "xcopa_codes=pd.read_csv('../mappings/xcopa-processed.10000.csv', index_col=0)\n",
    "xquad_codes=pd.read_csv('../mappings/xquad-processed.10000.csv', index_col=0)\n",
    "tydiqa_codes=pd.read_csv('../mappings/tydiqa-processed.10000.csv', index_col=0)\n",
    "xnli_codes=pd.read_csv('../mappings/xnli-processed.10000.csv', index_col=0)\n",
    "xtreme_codes=pd.read_csv('../mappings/xtreme-processed.10000.csv', index_col=0)\n",
    "xglue_codes=pd.read_csv('../mappings/xglue-processed.10000.csv', index_col=0)\n",
    "ud_codes=pd.read_csv('../mappings/ud-processed.tsv', sep='\\t', index_col=0)\n",
    "teddi_codes=pd.read_csv('../mappings/sample500.csv', index_col=0)\n",
    "mbert_codes=pd.read_csv('../mappings/mbertwiki-processed.10000.csv', index_col=0)\n",
    "bible_codes=pd.read_csv('../mappings/biblecorpus100-processed.10000.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "intense-diploma",
   "metadata": {},
   "outputs": [],
   "source": [
    "#manual substitutions for problematic cases\n",
    "ud_codes.loc[\"UD_Western_Armenian-ArmTDP.txt\"].at[\"ISO_6393\"]=\"hy\"\n",
    "mbert_codes.loc[\"armenian\"].at[\"ISO_6393\"]=\"hy\"\n",
    "mbert_codes.loc[\"vowiki-latest-pages-articles\"].at[\"ISO_6393\"]=\"vol\"\n",
    "bible_codes=bible_codes.drop([\"crp.txt\"])\n",
    "bible_codes.loc[\"jap.txt\"].at[\"ISO_6393\"]=\"jpn\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-payroll",
   "metadata": {},
   "source": [
    "Function for exracting the vectors according to the language codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "progressive-legend",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_l2v(dataset_codes):\n",
    "    #list of iso codes to query the l2v vectors:\n",
    "    codes=dataset_codes[\"ISO_6393\"].str.lower().tolist()\n",
    "    #codes=xcopa_codes.index.tolist()\n",
    "    \n",
    "    features = l2v.get_features(codes, \"syntax_knn\")\n",
    "    #features = l2v.get_features(codes, \"syntax_average\")\n",
    "\n",
    "    features_frame = pd.DataFrame.from_dict(features).transpose()\n",
    "    return (features_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-locking",
   "metadata": {},
   "source": [
    "Function for calculating the entropy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dress-fiber",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy(df): #frame with features\n",
    "    entropies=[]\n",
    "    for index in range(len(df.columns)): #We are processing column by column\n",
    "        p= np.ones(2)\n",
    "        freqs=df[index].to_numpy() #We convert the column to a Numpy array\n",
    "        #We extract the number of zeros, and the number of ones:\n",
    "        ones=len(freqs[freqs == 1])\n",
    "        zeros=len(freqs[freqs == 0])\n",
    "        #We calculate the prob:\n",
    "        p_ones = ones/len(freqs)    #Probability (relative frequency), e.g., 8/11, 3/11\n",
    "        p_zeros = zeros/len(freqs) \n",
    "        p[0]=p_ones\n",
    "        p[1]=p_zeros\n",
    "        p=p[p != 0]  #We extract only the values not equal to zero. (other wise we get a nan when aplying the algorithm)\n",
    "        H=-(p*np.log2(p)).sum()  #Entropy calculation\n",
    "        entropies.append(H)      #We store the entropy of each row in an array\n",
    "        #print(ones,zeros, p_ones, p_zeros, H)\n",
    "    return(entropies) #entropy feature-wise\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "southeast-residence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xcopa 0.5862770789920277\n",
      "xquad 0.5231360114362537\n",
      "tydiqa 0.626102007988479\n",
      "xnli 0.5570648270071721\n",
      "xtreme 0.6116017688923218\n",
      "xglue 0.5167326017164338\n",
      "ud 0.5674507124838225\n",
      "teddi 0.7063106049093542\n",
      "mbert 0.5590294778394355\n",
      "bibles 0.649122540960284\n"
     ]
    }
   ],
   "source": [
    "entropies_features=get_entropy(get_l2v(xcopa_codes)) #entropies feature-wise\n",
    "print(\"xcopa\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_l2v(xquad_codes)) #entropies feature-wise\n",
    "print(\"xquad\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_l2v(tydiqa_codes)) #entropies feature-wise\n",
    "print(\"tydiqa\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_l2v(xnli_codes)) #entropies feature-wise\n",
    "print(\"xnli\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_l2v(xtreme_codes)) #entropies feature-wise\n",
    "print(\"xtreme\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_l2v(xglue_codes)) #entropies feature-wise\n",
    "print(\"xglue\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_l2v(ud_codes)) #entropies feature-wise\n",
    "print(\"ud\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_l2v(teddi_codes)) #entropies feature-wise\n",
    "print(\"teddi\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_l2v(mbert_codes)) #entropies feature-wise\n",
    "print(\"mbert\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_l2v(bible_codes)) #entropies feature-wise\n",
    "print(\"bibles\", mean(entropies_features)) #mean of entropies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renewable-value",
   "metadata": {},
   "source": [
    "# 2. Typological index using word length as features "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-holder",
   "metadata": {},
   "source": [
    "---\n",
    "For Word Length: \n",
    "\n",
    "We have to create first vectors: eng 1 0 0 0 0 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "detected-hospital",
   "metadata": {},
   "outputs": [],
   "source": [
    "#***All the code to do the sampling, calculating measures, and finally obtain these tsvs, is not included in this notebook***\n",
    "teddi_10k=pd.read_csv('../wordlength_results/RESULTS_100LC_tokens/sample10000.tsv', sep='\\t',index_col=0)\n",
    "ud_10k=pd.read_csv('../wordlength_results/RESULTS_ud-processed_tokens/ud-processed.10000.stats.tsv', sep='\\t',index_col=0)\n",
    "bibles_10k=pd.read_csv('../wordlength_results/RESULTS_biblecorpus100-processed_tokens/biblecorpus100-processed.10000.stats.tsv', sep='\\t',index_col=0)\n",
    "xcopa_10k=pd.read_csv('../wordlength_results/RESULTS_xcopa-processed_tokens/xcopa-processed.10000.stats.tsv', sep='\\t',index_col=0)\n",
    "tydiqa_10k=pd.read_csv('../wordlength_results/RESULTS_tydiqa-processed_tokens/tydiqa-processed.10000.stats.tsv', sep='\\t',index_col=0)\n",
    "xquad_10k=pd.read_csv('../wordlength_results/RESULTS_xquad-processed_tokens/xquad-processed.10000.stats.tsv', sep='\\t',index_col=0)\n",
    "xnli_10k=pd.read_csv('../wordlength_results/RESULTS_xnli-processed_tokens/xnli-processed.10000.stats.tsv', sep='\\t',index_col=0)\n",
    "xglue_10k=pd.read_csv('../wordlength_results/RESULTS_xglue-processed_tokens/xglue-processed.10000.stats.tsv', sep='\\t',index_col=0)\n",
    "xtreme_10k=pd.read_csv('../wordlength_results/RESULTS_xtreme-processed_tokens/xtreme-processed.10000.stats.tsv', sep='\\t',index_col=0)\n",
    "mbert_10k=pd.read_csv('../wordlength_results/RESULTS_mbertwiki-processed_tokens/mbertwiki-processed.10000.stats.tsv', sep='\\t',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertical-flexibility",
   "metadata": {},
   "source": [
    "Function for creating binary vectors, using the word length. Example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "special-soccer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordlength_vectors(dataset):\n",
    "    #For each language initialize an array of elements (the maximum possoble word length in all lamguages)\n",
    "    langs=dataset.index.tolist()\n",
    "    vectors_hash={}\n",
    "    for l in langs:    \n",
    "\n",
    "        binary_vector= np.zeros(11)\n",
    "\n",
    "        #wordlength=dataset.loc[l]['Avg_length']\n",
    "        wordlength=dataset.loc[l]['Median_length'] #for median\n",
    "\n",
    "        #if Avg_length it is betwwen 0 and 1: assign 1 to the first bin (element of the array), and so on:\n",
    "        \n",
    "        #binary_vector[(round(wordlength))-1]=1 #binarization using round numbers\n",
    "        binary_vector[(int(wordlength))-1]=1  #binarization using the integer part of the number\n",
    "        #print (l, wordlength, round(wordlength), binary_vector)\n",
    "        #print (l, wordlength, binary_vector)\n",
    "        \n",
    "        vectors_hash[l]= binary_vector\n",
    "        \n",
    "    \n",
    "    return(pd.DataFrame.from_dict(vectors_hash).transpose()) #a dataframe with one vector per row "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identified-scanning",
   "metadata": {},
   "source": [
    "Results using binarization based on the integer part of the wod length. Example:\n",
    "\n",
    "If a language has word_length 4.9, its vector will have  \"1\" in the position 4.  Bins like 0 -1, 1 - 2, etc.\n",
    "\n",
    "0 0 0 1 0 0 0 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cross-suspension",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xcopa 0.33574829724076866\n",
      "xquad 0.3198532291498096\n",
      "tydiqa 0.36103779596241575\n",
      "xnli 0.33874266714204293\n",
      "xtreme 0.330476369909514\n",
      "xglue 0.31592521410599766\n",
      "ud 0.3423811073664347\n",
      "teddi 0.3543710835203777\n",
      "mbert 0.3467294224091484\n",
      "bibles 0.31059402896766475\n"
     ]
    }
   ],
   "source": [
    "entropies_features=get_entropy(get_wordlength_vectors(xcopa_10k)) #entropies feature-wise\n",
    "print(\"xcopa\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(xquad_10k)) #entropies feature-wise\n",
    "print(\"xquad\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(tydiqa_10k)) #entropies feature-wise\n",
    "print(\"tydiqa\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(xnli_10k)) #entropies feature-wise\n",
    "print(\"xnli\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(xtreme_10k)) #entropies feature-wise\n",
    "print(\"xtreme\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(xglue_10k)) #entropies feature-wise\n",
    "print(\"xglue\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(ud_10k)) #entropies feature-wise\n",
    "print(\"ud\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(teddi_10k)) #entropies feature-wise\n",
    "print(\"teddi\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(mbert_10k)) #entropies feature-wise\n",
    "print(\"mbert\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(bibles_10k)) #entropies feature-wise\n",
    "print(\"bibles\", mean(entropies_features)) #mean of entropies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-status",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "- Results using the rounded version of the the word length for the binarization. Example:\n",
    "\n",
    "If a language has word_length 4.9, its vector will have \"1\" in the position 5.\n",
    "\n",
    "0 0 0 0 1 0 0 ...\n",
    "\n",
    "If a language has word_length 4.1, its vector will have \"1\" in the position 4.\n",
    "\n",
    "0 0 0 1 0 0 0 ...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "royal-hurricane",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xcopa 0.33574829724076866\n",
      "xquad 0.3198532291498096\n",
      "tydiqa 0.36103779596241575\n",
      "xnli 0.33874266714204293\n",
      "xtreme 0.330476369909514\n",
      "xglue 0.31592521410599766\n",
      "ud 0.3423811073664347\n",
      "teddi 0.3543710835203777\n",
      "mbert 0.3467294224091484\n",
      "bibles 0.31059402896766475\n"
     ]
    }
   ],
   "source": [
    "entropies_features=get_entropy(get_wordlength_vectors(xcopa_10k)) #entropies feature-wise\n",
    "print(\"xcopa\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(xquad_10k)) #entropies feature-wise\n",
    "print(\"xquad\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(tydiqa_10k)) #entropies feature-wise\n",
    "print(\"tydiqa\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(xnli_10k)) #entropies feature-wise\n",
    "print(\"xnli\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(xtreme_10k)) #entropies feature-wise\n",
    "print(\"xtreme\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(xglue_10k)) #entropies feature-wise\n",
    "print(\"xglue\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(ud_10k)) #entropies feature-wise\n",
    "print(\"ud\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(teddi_10k)) #entropies feature-wise\n",
    "print(\"teddi\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(mbert_10k)) #entropies feature-wise\n",
    "print(\"mbert\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(bibles_10k)) #entropies feature-wise\n",
    "print(\"bibles\", mean(entropies_features)) #mean of entropies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-occupation",
   "metadata": {},
   "source": [
    "----\n",
    "Binarization using the median:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "veterinary-italian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xcopa 0.33574829724076866\n",
      "xquad 0.3198532291498096\n",
      "tydiqa 0.36103779596241575\n",
      "xnli 0.33874266714204293\n",
      "xtreme 0.330476369909514\n",
      "xglue 0.31592521410599766\n",
      "ud 0.3423811073664347\n",
      "teddi 0.3543710835203777\n",
      "mbert 0.3467294224091484\n",
      "bibles 0.31059402896766475\n"
     ]
    }
   ],
   "source": [
    "entropies_features=get_entropy(get_wordlength_vectors(xcopa_10k)) #entropies feature-wise\n",
    "print(\"xcopa\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(xquad_10k)) #entropies feature-wise\n",
    "print(\"xquad\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(tydiqa_10k)) #entropies feature-wise\n",
    "print(\"tydiqa\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(xnli_10k)) #entropies feature-wise\n",
    "print(\"xnli\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(xtreme_10k)) #entropies feature-wise\n",
    "print(\"xtreme\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(xglue_10k)) #entropies feature-wise\n",
    "print(\"xglue\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(ud_10k)) #entropies feature-wise\n",
    "print(\"ud\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(teddi_10k)) #entropies feature-wise\n",
    "print(\"teddi\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(mbert_10k)) #entropies feature-wise\n",
    "print(\"mbert\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(bibles_10k)) #entropies feature-wise\n",
    "print(\"bibles\", mean(entropies_features)) #mean of entropies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-arizona",
   "metadata": {},
   "source": [
    "---\n",
    "*Using varied size of bins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cardiovascular-baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordlength_vectors2(dataset, binsize):\n",
    "    #For each language initialize an array of elements (the maximum possoble word length in all lamguages)\n",
    "    \n",
    "    bins=np.arange(1, 11.2, binsize) #[ 1. ,  1.1,  1.2,  1.3... ]\n",
    "    \n",
    "    langs=dataset.index.tolist()\n",
    "    vectors_hash={}\n",
    "    \n",
    "    for l in langs:    \n",
    "\n",
    "        binary_vector= np.zeros(len(bins))\n",
    "\n",
    "        wordlength=dataset.loc[l]['Avg_length']\n",
    "        index=len(np.arange(1, wordlength, binsize)) #we partition the word length in the same bins, the total size of the array is the index for putting a 1 in the binary vector\n",
    "        #print(l, wordlength, index)\n",
    "        #index=bins.round(decimals=2).tolist().index(round(wordlength,1)) #locate the index that has that word length (index starts at zero)\n",
    "       \n",
    "\n",
    "        #if Avg_length it is betwwen 0 and 1: assign 1 to the first bin (element of the array), and so on:\n",
    "        \n",
    "        binary_vector[index-1]=1  \n",
    "        \n",
    "        vectors_hash[l]= binary_vector\n",
    "        \n",
    "    \n",
    "    return(pd.DataFrame.from_dict(vectors_hash).transpose()) #a dataframe with one vector per row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "twenty-network",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bin Size: 1\n",
      "features: 11\n"
     ]
    }
   ],
   "source": [
    "print(\"Bin Size: 1\")\n",
    "\n",
    "print(\"features:\", str(len(np.arange(1, 11.2, 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "breathing-auckland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xcopa 0.36103779596241575\n",
      "xquad 0.34093043571111287\n",
      "tydiqa 0.3433145652165989\n",
      "xnli 0.33874266714204293\n",
      "xtreme 0.31093208189005345\n",
      "xglue 0.3072073812912939\n",
      "ud 0.34890225383819307\n",
      "teddi 0.36874805539335226\n",
      "mbert 0.32349322087652765\n",
      "bibles 0.31080272059778274\n"
     ]
    }
   ],
   "source": [
    "entropies_features=get_entropy(get_wordlength_vectors2(xcopa_10k, 1)) #entropies feature-wise\n",
    "print(\"xcopa\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(xquad_10k, 1)) #entropies feature-wise\n",
    "print(\"xquad\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(tydiqa_10k, 1)) #entropies feature-wise\n",
    "print(\"tydiqa\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(xnli_10k, 1)) #entropies feature-wise\n",
    "print(\"xnli\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(xtreme_10k, 1)) #entropies feature-wise\n",
    "print(\"xtreme\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(xglue_10k, 1)) #entropies feature-wise\n",
    "print(\"xglue\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(ud_10k, 1)) #entropies feature-wise\n",
    "print(\"ud\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(teddi_10k, 1)) #entropies feature-wise\n",
    "print(\"teddi\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(mbert_10k, 1)) #entropies feature-wise\n",
    "print(\"mbert\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(bibles_10k, 1)) #entropies feature-wise\n",
    "print(\"bibles\", mean(entropies_features)) #mean of entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "welsh-internship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bin Size: 0.5\n",
      "features: 21\n"
     ]
    }
   ],
   "source": [
    "print(\"Bin Size: 0.5\")\n",
    "\n",
    "print(\"features:\", str(len(np.arange(1, 11.2, 0.5))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "false-liver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xcopa 0.20236191626308056\n",
      "xquad 0.2110937317508031\n",
      "tydiqa 0.19307831920574794\n",
      "xnli 0.21626726784164793\n",
      "xtreme 0.20752813424837216\n",
      "xglue 0.19882210764139027\n",
      "ud 0.2298559516001784\n",
      "teddi 0.2366665678889429\n",
      "mbert 0.2157086068330691\n",
      "bibles 0.21448215955437389\n"
     ]
    }
   ],
   "source": [
    "entropies_features=get_entropy(get_wordlength_vectors2(xcopa_10k, 0.5)) #entropies feature-wise\n",
    "print(\"xcopa\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(xquad_10k, 0.5)) #entropies feature-wise\n",
    "print(\"xquad\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(tydiqa_10k, 0.5)) #entropies feature-wise\n",
    "print(\"tydiqa\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(xnli_10k, 0.5)) #entropies feature-wise\n",
    "print(\"xnli\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(xtreme_10k, 0.5)) #entropies feature-wise\n",
    "print(\"xtreme\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(xglue_10k, 0.5)) #entropies feature-wise\n",
    "print(\"xglue\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(ud_10k, 0.5)) #entropies feature-wise\n",
    "print(\"ud\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(teddi_10k, 0.5)) #entropies feature-wise\n",
    "print(\"teddi\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(mbert_10k, 0.5)) #entropies feature-wise\n",
    "print(\"mbert\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(bibles_10k, 0.5)) #entropies feature-wise\n",
    "print(\"bibles\", mean(entropies_features)) #mean of entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "worthy-firmware",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_wordlength_vectors2(teddi_10k, 0.5).to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "stretch-savings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bin Size: 0.1\n",
      "features: 102\n"
     ]
    }
   ],
   "source": [
    "print(\"Bin Size: 0.1\")\n",
    "\n",
    "print(\"features:\", str(len(np.arange(1, 11.2, 0.1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "catholic-eligibility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xcopa 0.04739673388369262\n",
      "xquad 0.04868433532983926\n",
      "tydiqa 0.043574076271849775\n",
      "xnli 0.05059000844932728\n",
      "xtreme 0.05680398812695187\n",
      "xglue 0.05004464749022719\n",
      "ud 0.06541596102694645\n",
      "teddi 0.06582863572653203\n",
      "mbert 0.06372981819372056\n",
      "bibles 0.06286102023805801\n"
     ]
    }
   ],
   "source": [
    "entropies_features=get_entropy(get_wordlength_vectors2(xcopa_10k, 0.1)) #entropies feature-wise\n",
    "print(\"xcopa\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(xquad_10k, 0.1)) #entropies feature-wise\n",
    "print(\"xquad\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(tydiqa_10k, 0.1)) #entropies feature-wise\n",
    "print(\"tydiqa\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(xnli_10k, 0.1)) #entropies feature-wise\n",
    "print(\"xnli\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(xtreme_10k, 0.1)) #entropies feature-wise\n",
    "print(\"xtreme\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(xglue_10k, 0.1)) #entropies feature-wise\n",
    "print(\"xglue\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(ud_10k, 0.1)) #entropies feature-wise\n",
    "print(\"ud\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(teddi_10k, 0.1)) #entropies feature-wise\n",
    "print(\"teddi\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(mbert_10k, 0.1)) #entropies feature-wise\n",
    "print(\"mbert\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(bibles_10k, 0.1)) #entropies feature-wise\n",
    "print(\"bibles\", mean(entropies_features)) #mean of entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "advance-liberty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bin Size: 0.9\n",
      "features: 12\n"
     ]
    }
   ],
   "source": [
    "print(\"Bin Size: 0.9\")\n",
    "\n",
    "print(\"features:\", str(len(np.arange(1, 11.2, 0.9))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "traditional-display",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xcopa 0.2957499324090942\n",
      "xquad 0.3017539109227398\n",
      "tydiqa 0.3309513129655478\n",
      "xnli 0.3105141115468727\n",
      "xtreme 0.3063741359892156\n",
      "xglue 0.27184620200236054\n",
      "ud 0.3294031922062626\n",
      "teddi 0.3411857535093596\n",
      "mbert 0.30593440698272845\n",
      "bibles 0.312136655159478\n"
     ]
    }
   ],
   "source": [
    "entropies_features=get_entropy(get_wordlength_vectors2(xcopa_10k, 0.9)) #entropies feature-wise\n",
    "print(\"xcopa\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(xquad_10k, 0.9)) #entropies feature-wise\n",
    "print(\"xquad\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(tydiqa_10k, 0.9)) #entropies feature-wise\n",
    "print(\"tydiqa\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(xnli_10k, 0.9)) #entropies feature-wise\n",
    "print(\"xnli\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(xtreme_10k, 0.9)) #entropies feature-wise\n",
    "print(\"xtreme\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(xglue_10k, 0.9)) #entropies feature-wise\n",
    "print(\"xglue\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(ud_10k, 0.9)) #entropies feature-wise\n",
    "print(\"ud\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(teddi_10k, 0.9)) #entropies feature-wise\n",
    "print(\"teddi\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(mbert_10k, 0.9)) #entropies feature-wise\n",
    "print(\"mbert\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(bibles_10k, 0.9)) #entropies feature-wise\n",
    "print(\"bibles\", mean(entropies_features)) #mean of entropies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "moral-power",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bin Size: 0.8\n",
      "features: 13\n"
     ]
    }
   ],
   "source": [
    "print(\"Bin Size: 0.8\")\n",
    "\n",
    "print(\"features:\", str(len(np.arange(1, 11.2, 0.8))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "voluntary-century",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xcopa 0.30549351966050564\n",
      "xquad 0.3079073800242382\n",
      "tydiqa 0.29049693979866065\n",
      "xnli 0.2866284106586517\n",
      "xtreme 0.28044117924825046\n",
      "xglue 0.29178050174694803\n",
      "ud 0.3133636403369355\n",
      "teddi 0.3326760338726688\n",
      "mbert 0.2995274910619819\n",
      "bibles 0.29442307628370074\n"
     ]
    }
   ],
   "source": [
    "entropies_features=get_entropy(get_wordlength_vectors2(xcopa_10k, 0.8)) #entropies feature-wise\n",
    "print(\"xcopa\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(xquad_10k, 0.8)) #entropies feature-wise\n",
    "print(\"xquad\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(tydiqa_10k, 0.8)) #entropies feature-wise\n",
    "print(\"tydiqa\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(xnli_10k, 0.8)) #entropies feature-wise\n",
    "print(\"xnli\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(xtreme_10k, 0.8)) #entropies feature-wise\n",
    "print(\"xtreme\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(xglue_10k, 0.8)) #entropies feature-wise\n",
    "print(\"xglue\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(ud_10k, 0.8)) #entropies feature-wise\n",
    "print(\"ud\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(teddi_10k, 0.8)) #entropies feature-wise\n",
    "print(\"teddi\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(mbert_10k, 0.8)) #entropies feature-wise\n",
    "print(\"mbert\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(bibles_10k, 0.8)) #entropies feature-wise\n",
    "print(\"bibles\", mean(entropies_features)) #mean of entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-sapphire",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encouraging-perception",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
