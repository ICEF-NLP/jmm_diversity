{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "geological-investigator",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "#from nltk.util import ngrams\n",
    "\n",
    "import pandas as pd\n",
    "#import seaborn as sb\n",
    "\n",
    "import sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [15, 6]\n",
    "from pandas import DataFrame\n",
    "import lang2vec.lang2vec as l2v\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-tissue",
   "metadata": {},
   "source": [
    "# 1. Typological index using syntactic features (lang2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "irish-porcelain",
   "metadata": {},
   "outputs": [],
   "source": [
    "xcopa_codes=pd.read_csv('../mappings/xcopa-processed.10000.csv', index_col=0)\n",
    "xquad_codes=pd.read_csv('../mappings/xquad-processed.10000.csv', index_col=0)\n",
    "tydiqa_codes=pd.read_csv('../mappings/tydiqa-processed.10000.csv', index_col=0)\n",
    "xnli_codes=pd.read_csv('../mappings/xnli-processed.10000.csv', index_col=0)\n",
    "xtreme_codes=pd.read_csv('../mappings/xtreme-processed.10000.csv', index_col=0)\n",
    "xglue_codes=pd.read_csv('../mappings/xglue-processed.10000.csv', index_col=0)\n",
    "ud_codes=pd.read_csv('../mappings/ud-processed.tsv', sep='\\t', index_col=0)\n",
    "teddi_codes=pd.read_csv('../mappings/sample500.csv', index_col=0)\n",
    "mbert_codes=pd.read_csv('../mappings/mbertwiki-processed.10000.csv', index_col=0)\n",
    "bible_codes=pd.read_csv('../mappings/biblecorpus100-processed.10000.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "occupational-power",
   "metadata": {},
   "outputs": [],
   "source": [
    "#manual substitutions for problematic cases\n",
    "ud_codes.loc[\"UD_Western_Armenian-ArmTDP.txt\"].at[\"ISO_6393\"]=\"hy\"\n",
    "mbert_codes.loc[\"armenian\"].at[\"ISO_6393\"]=\"hy\"\n",
    "mbert_codes.loc[\"vowiki-latest-pages-articles\"].at[\"ISO_6393\"]=\"vol\"\n",
    "bible_codes=bible_codes.drop([\"crp.txt\"])\n",
    "bible_codes.loc[\"jap.txt\"].at[\"ISO_6393\"]=\"jpn\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-technique",
   "metadata": {},
   "source": [
    "Function for exracting the vectors according to the language codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "smooth-registrar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_l2v(dataset_codes):\n",
    "    #list of iso codes to query the l2v vectors:\n",
    "    codes=dataset_codes[\"ISO_6393\"].str.lower().tolist()\n",
    "    #codes=xcopa_codes.index.tolist()\n",
    "    \n",
    "    features = l2v.get_features(codes, \"syntax_knn\")\n",
    "    #features = l2v.get_features(codes, \"syntax_average\")\n",
    "\n",
    "    features_frame = pd.DataFrame.from_dict(features).transpose()\n",
    "    return (features_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-indian",
   "metadata": {},
   "source": [
    "Function for calculating the entropy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "authorized-target",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy(df): #frame with features\n",
    "    entropies=[]\n",
    "    for index in range(len(df.columns)): #We are processing column by column\n",
    "        p= np.ones(2)\n",
    "        freqs=df[index].to_numpy() #We convert the column to a Numpy array\n",
    "        #We extract the number of zeros, and the number of ones:\n",
    "        ones=len(freqs[freqs == 1])\n",
    "        zeros=len(freqs[freqs == 0])\n",
    "        #We calculate the prob:\n",
    "        p_ones = ones/len(freqs)    #Probability (relative frequency), e.g., 8/11, 3/11\n",
    "        p_zeros = zeros/len(freqs) \n",
    "        p[0]=p_ones\n",
    "        p[1]=p_zeros\n",
    "        p=p[p != 0]  #We extract only the values not equal to zero. (other wise we get a nan when aplying the algorithm)\n",
    "        H=-(p*np.log2(p)).sum()  #Entropy calculation\n",
    "        entropies.append(H)      #We store the entropy of each row in an array\n",
    "        #print(ones,zeros, p_ones, p_zeros, H)\n",
    "    return(entropies) #entropy feature-wise\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "local-religion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xcopa 0.5862770789920277\n",
      "xquad 0.5231360114362537\n",
      "tydiqa 0.626102007988479\n",
      "xnli 0.5570648270071721\n",
      "xtreme 0.6116017688923218\n",
      "xglue 0.5167326017164338\n",
      "ud 0.5674507124838225\n",
      "teddi 0.7063106049093542\n",
      "mbert 0.5590294778394355\n",
      "bibles 0.649122540960284\n"
     ]
    }
   ],
   "source": [
    "entropies_features=get_entropy(get_l2v(xcopa_codes)) #entropies feature-wise\n",
    "print(\"xcopa\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_l2v(xquad_codes)) #entropies feature-wise\n",
    "print(\"xquad\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_l2v(tydiqa_codes)) #entropies feature-wise\n",
    "print(\"tydiqa\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_l2v(xnli_codes)) #entropies feature-wise\n",
    "print(\"xnli\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_l2v(xtreme_codes)) #entropies feature-wise\n",
    "print(\"xtreme\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_l2v(xglue_codes)) #entropies feature-wise\n",
    "print(\"xglue\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_l2v(ud_codes)) #entropies feature-wise\n",
    "print(\"ud\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_l2v(teddi_codes)) #entropies feature-wise\n",
    "print(\"teddi\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_l2v(mbert_codes)) #entropies feature-wise\n",
    "print(\"mbert\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_l2v(bible_codes)) #entropies feature-wise\n",
    "print(\"bibles\", mean(entropies_features)) #mean of entropies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-combination",
   "metadata": {},
   "source": [
    "# 2. Typological index using word length as features "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-ownership",
   "metadata": {},
   "source": [
    "---\n",
    "For Word Length: \n",
    "\n",
    "We have to create first vectors: eng 1 0 0 0 0 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "perfect-cologne",
   "metadata": {},
   "outputs": [],
   "source": [
    "#***All the code to do the sampling, calculating measures, and finally obtain these tsvs, is not included in this notebook***\n",
    "teddi_10k=pd.read_csv('../wordlength_results/RESULTS_100LC_tokens/sample10000_logographicadjusted.tsv', sep='\\t',index_col=0)\n",
    "ud_10k=pd.read_csv('../wordlength_results/RESULTS_ud-processed_tokens/ud-processed.10000.stats_adjusted.tsv', sep='\\t',index_col=0)\n",
    "bibles_10k=pd.read_csv('../wordlength_results/RESULTS_biblecorpus100-processed_tokens/biblecorpus100-processed.10000.stats_adjusted.tsv', sep='\\t',index_col=0)\n",
    "xcopa_10k=pd.read_csv('../wordlength_results/RESULTS_xcopa-processed_tokens/xcopa-processed.10000.stats_adjusted.tsv', sep='\\t',index_col=0)\n",
    "tydiqa_10k=pd.read_csv('../wordlength_results/RESULTS_tydiqa-processed_tokens/tydiqa-processed.10000.stats_adjusted.tsv', sep='\\t',index_col=0)\n",
    "xquad_10k=pd.read_csv('../wordlength_results/RESULTS_xquad-processed_tokens/xquad-processed.10000.stats_adjusted.tsv', sep='\\t',index_col=0)\n",
    "xnli_10k=pd.read_csv('../wordlength_results/RESULTS_xnli-processed_tokens/xnli-processed.10000.stats_adjusted.tsv', sep='\\t',index_col=0)\n",
    "xglue_10k=pd.read_csv('../wordlength_results/RESULTS_xglue-processed_tokens/xglue-processed.10000.stats_adjusted.tsv', sep='\\t',index_col=0)\n",
    "xtreme_10k=pd.read_csv('../wordlength_results/RESULTS_xtreme-processed_tokens/xtreme-processed.10000.stats_adjusted.tsv', sep='\\t',index_col=0)\n",
    "mbert_10k=pd.read_csv('../wordlength_results/RESULTS_mbertwiki-processed_tokens/mbertwiki-processed.10000.stats_adjusted.tsv', sep='\\t',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-reset",
   "metadata": {},
   "source": [
    "Function for creating binary vectors, using the word length. Example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cross-nature",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordlength_vectors(dataset):\n",
    "    #For each language initialize an array of elements (the maximum possoble word length in all lamguages)\n",
    "    langs=dataset.index.tolist()\n",
    "    vectors_hash={}\n",
    "    for l in langs:    \n",
    "\n",
    "        binary_vector= np.zeros(11)\n",
    "\n",
    "        #wordlength=dataset.loc[l]['Avg_length']\n",
    "        wordlength=dataset.loc[l]['Median_length'] #for median\n",
    "\n",
    "        #if Avg_length it is betwwen 0 and 1: assign 1 to the first bin (element of the array), and so on:\n",
    "        \n",
    "        #binary_vector[(round(wordlength))-1]=1 #binarization using round numbers\n",
    "        binary_vector[(int(wordlength))-1]=1  #binarization using the integer part of the number\n",
    "        #print (l, wordlength, round(wordlength), binary_vector)\n",
    "        #print (l, wordlength, binary_vector)\n",
    "        \n",
    "        vectors_hash[l]= binary_vector\n",
    "        \n",
    "    \n",
    "    return(pd.DataFrame.from_dict(vectors_hash).transpose()) #a dataframe with one vector per row "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-class",
   "metadata": {},
   "source": [
    "Results using binarization based on the integer part of the wod length. Example:\n",
    "\n",
    "If a language has word_length 4.9, its vector will have  \"1\" in the position 4.  Bins like 0 -1, 1 - 2, etc.\n",
    "\n",
    "0 0 0 1 0 0 0 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "heard-dancing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xcopa 0.33574829724076866\n",
      "xquad 0.3198532291498096\n",
      "tydiqa 0.36103779596241575\n",
      "xnli 0.33874266714204293\n",
      "xtreme 0.330476369909514\n",
      "xglue 0.31592521410599766\n",
      "ud 0.3423811073664347\n",
      "teddi 0.3543710835203777\n",
      "mbert 0.3467294224091484\n",
      "bibles 0.31059402896766475\n"
     ]
    }
   ],
   "source": [
    "entropies_features=get_entropy(get_wordlength_vectors(xcopa_10k)) #entropies feature-wise\n",
    "print(\"xcopa\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(xquad_10k)) #entropies feature-wise\n",
    "print(\"xquad\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(tydiqa_10k)) #entropies feature-wise\n",
    "print(\"tydiqa\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(xnli_10k)) #entropies feature-wise\n",
    "print(\"xnli\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(xtreme_10k)) #entropies feature-wise\n",
    "print(\"xtreme\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(xglue_10k)) #entropies feature-wise\n",
    "print(\"xglue\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(ud_10k)) #entropies feature-wise\n",
    "print(\"ud\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(teddi_10k)) #entropies feature-wise\n",
    "print(\"teddi\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(mbert_10k)) #entropies feature-wise\n",
    "print(\"mbert\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(bibles_10k)) #entropies feature-wise\n",
    "print(\"bibles\", mean(entropies_features)) #mean of entropies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-feature",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "- Results using the rounded version of the the word length for the binarization. Example:\n",
    "\n",
    "If a language has word_length 4.9, its vector will have \"1\" in the position 5.\n",
    "\n",
    "0 0 0 0 1 0 0 ...\n",
    "\n",
    "If a language has word_length 4.1, its vector will have \"1\" in the position 4.\n",
    "\n",
    "0 0 0 1 0 0 0 ...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "protecting-relationship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xcopa 0.33574829724076866\n",
      "xquad 0.3198532291498096\n",
      "tydiqa 0.36103779596241575\n",
      "xnli 0.33874266714204293\n",
      "xtreme 0.330476369909514\n",
      "xglue 0.31592521410599766\n",
      "ud 0.3423811073664347\n",
      "teddi 0.3543710835203777\n",
      "mbert 0.3467294224091484\n",
      "bibles 0.31059402896766475\n"
     ]
    }
   ],
   "source": [
    "entropies_features=get_entropy(get_wordlength_vectors(xcopa_10k)) #entropies feature-wise\n",
    "print(\"xcopa\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(xquad_10k)) #entropies feature-wise\n",
    "print(\"xquad\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(tydiqa_10k)) #entropies feature-wise\n",
    "print(\"tydiqa\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(xnli_10k)) #entropies feature-wise\n",
    "print(\"xnli\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(xtreme_10k)) #entropies feature-wise\n",
    "print(\"xtreme\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(xglue_10k)) #entropies feature-wise\n",
    "print(\"xglue\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(ud_10k)) #entropies feature-wise\n",
    "print(\"ud\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(teddi_10k)) #entropies feature-wise\n",
    "print(\"teddi\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(mbert_10k)) #entropies feature-wise\n",
    "print(\"mbert\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(bibles_10k)) #entropies feature-wise\n",
    "print(\"bibles\", mean(entropies_features)) #mean of entropies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-chosen",
   "metadata": {},
   "source": [
    "----\n",
    "Binarization using the median:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "painted-triumph",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xcopa 0.33574829724076866\n",
      "xquad 0.3198532291498096\n",
      "tydiqa 0.36103779596241575\n",
      "xnli 0.33874266714204293\n",
      "xtreme 0.330476369909514\n",
      "xglue 0.31592521410599766\n",
      "ud 0.3423811073664347\n",
      "teddi 0.3543710835203777\n",
      "mbert 0.3467294224091484\n",
      "bibles 0.31059402896766475\n"
     ]
    }
   ],
   "source": [
    "entropies_features=get_entropy(get_wordlength_vectors(xcopa_10k)) #entropies feature-wise\n",
    "print(\"xcopa\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(xquad_10k)) #entropies feature-wise\n",
    "print(\"xquad\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(tydiqa_10k)) #entropies feature-wise\n",
    "print(\"tydiqa\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(xnli_10k)) #entropies feature-wise\n",
    "print(\"xnli\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(xtreme_10k)) #entropies feature-wise\n",
    "print(\"xtreme\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(xglue_10k)) #entropies feature-wise\n",
    "print(\"xglue\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(ud_10k)) #entropies feature-wise\n",
    "print(\"ud\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(teddi_10k)) #entropies feature-wise\n",
    "print(\"teddi\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(mbert_10k)) #entropies feature-wise\n",
    "print(\"mbert\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors(bibles_10k)) #entropies feature-wise\n",
    "print(\"bibles\", mean(entropies_features)) #mean of entropies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-exposure",
   "metadata": {},
   "source": [
    "---\n",
    "*Using varied size of bins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "herbal-greensboro",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordlength_vectors2(dataset, binsize):\n",
    "    #For each language initialize an array of elements (the maximum possoble word length in all lamguages)\n",
    "    \n",
    "    bins=np.arange(1, 11.2, binsize) #[ 1. ,  1.1,  1.2,  1.3... ]\n",
    "    \n",
    "    langs=dataset.index.tolist()\n",
    "    vectors_hash={}\n",
    "    \n",
    "    for l in langs:    \n",
    "\n",
    "        binary_vector= np.zeros(len(bins))\n",
    "\n",
    "        wordlength=dataset.loc[l]['Avg_length']\n",
    "        index=len(np.arange(1, wordlength, binsize)) #we partition the word length in the same bins, the total size of the array is the index for putting a 1 in the binary vector\n",
    "        #print(l, wordlength, index)\n",
    "        #index=bins.round(decimals=2).tolist().index(round(wordlength,1)) #locate the index that has that word length (index starts at zero)\n",
    "       \n",
    "\n",
    "        #if Avg_length it is betwwen 0 and 1: assign 1 to the first bin (element of the array), and so on:\n",
    "        \n",
    "        binary_vector[index-1]=1  \n",
    "        \n",
    "        vectors_hash[l]= binary_vector\n",
    "        \n",
    "    \n",
    "    return(pd.DataFrame.from_dict(vectors_hash).transpose()) #a dataframe with one vector per row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "attempted-visitor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bin Size: 1\n",
      "features: 11\n"
     ]
    }
   ],
   "source": [
    "print(\"Bin Size: 1\")\n",
    "\n",
    "print(\"features:\", str(len(np.arange(1, 11.2, 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "muslim-minnesota",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xcopa 0.33574829724076866\n",
      "xquad 0.31797033139358055\n",
      "tydiqa 0.3433145652165989\n",
      "xnli 0.32074805380686433\n",
      "xtreme 0.31093208189005345\n",
      "xglue 0.29725431428286975\n",
      "ud 0.3370770502591743\n",
      "teddi 0.3611454857345919\n",
      "mbert 0.31640987282092636\n",
      "bibles 0.30242304086676636\n"
     ]
    }
   ],
   "source": [
    "entropies_features=get_entropy(get_wordlength_vectors2(xcopa_10k, 1)) #entropies feature-wise\n",
    "print(\"xcopa\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(xquad_10k, 1)) #entropies feature-wise\n",
    "print(\"xquad\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(tydiqa_10k, 1)) #entropies feature-wise\n",
    "print(\"tydiqa\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(xnli_10k, 1)) #entropies feature-wise\n",
    "print(\"xnli\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(xtreme_10k, 1)) #entropies feature-wise\n",
    "print(\"xtreme\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(xglue_10k, 1)) #entropies feature-wise\n",
    "print(\"xglue\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(ud_10k, 1)) #entropies feature-wise\n",
    "print(\"ud\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(teddi_10k, 1)) #entropies feature-wise\n",
    "print(\"teddi\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(mbert_10k, 1)) #entropies feature-wise\n",
    "print(\"mbert\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(bibles_10k, 1)) #entropies feature-wise\n",
    "print(\"bibles\", mean(entropies_features)) #mean of entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "amateur-wisdom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bin Size: 0.5\n",
      "features: 21\n"
     ]
    }
   ],
   "source": [
    "print(\"Bin Size: 0.5\")\n",
    "\n",
    "print(\"features:\", str(len(np.arange(1, 11.2, 0.5))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "alert-earthquake",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xcopa 0.20236191626308056\n",
      "xquad 0.2110937317508031\n",
      "tydiqa 0.19307831920574794\n",
      "xnli 0.20959064100879377\n",
      "xtreme 0.20837922613984278\n",
      "xglue 0.19360859635126332\n",
      "ud 0.2236461098354233\n",
      "teddi 0.23474913097437664\n",
      "mbert 0.21359215870427253\n",
      "bibles 0.21033574347935505\n"
     ]
    }
   ],
   "source": [
    "entropies_features=get_entropy(get_wordlength_vectors2(xcopa_10k, 0.5)) #entropies feature-wise\n",
    "print(\"xcopa\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(xquad_10k, 0.5)) #entropies feature-wise\n",
    "print(\"xquad\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(tydiqa_10k, 0.5)) #entropies feature-wise\n",
    "print(\"tydiqa\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(xnli_10k, 0.5)) #entropies feature-wise\n",
    "print(\"xnli\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(xtreme_10k, 0.5)) #entropies feature-wise\n",
    "print(\"xtreme\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(xglue_10k, 0.5)) #entropies feature-wise\n",
    "print(\"xglue\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(ud_10k, 0.5)) #entropies feature-wise\n",
    "print(\"ud\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(teddi_10k, 0.5)) #entropies feature-wise\n",
    "print(\"teddi\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(mbert_10k, 0.5)) #entropies feature-wise\n",
    "print(\"mbert\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(bibles_10k, 0.5)) #entropies feature-wise\n",
    "print(\"bibles\", mean(entropies_features)) #mean of entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "comprehensive-anatomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_wordlength_vectors2(teddi_10k, 0.5).to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "growing-benefit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bin Size: 0.1\n",
      "features: 102\n"
     ]
    }
   ],
   "source": [
    "print(\"Bin Size: 0.1\")\n",
    "\n",
    "print(\"features:\", str(len(np.arange(1, 11.2, 0.1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "partial-trade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xcopa 0.04739673388369262\n",
      "xquad 0.04868433532983926\n",
      "tydiqa 0.043574076271849775\n",
      "xnli 0.05059000844932728\n",
      "xtreme 0.05680398812695187\n",
      "xglue 0.05004464749022719\n",
      "ud 0.06485724184042974\n",
      "teddi 0.06560651691019755\n",
      "mbert 0.06344832411737715\n",
      "bibles 0.06254494978857637\n"
     ]
    }
   ],
   "source": [
    "entropies_features=get_entropy(get_wordlength_vectors2(xcopa_10k, 0.1)) #entropies feature-wise\n",
    "print(\"xcopa\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(xquad_10k, 0.1)) #entropies feature-wise\n",
    "print(\"xquad\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(tydiqa_10k, 0.1)) #entropies feature-wise\n",
    "print(\"tydiqa\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(xnli_10k, 0.1)) #entropies feature-wise\n",
    "print(\"xnli\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(xtreme_10k, 0.1)) #entropies feature-wise\n",
    "print(\"xtreme\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(xglue_10k, 0.1)) #entropies feature-wise\n",
    "print(\"xglue\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(ud_10k, 0.1)) #entropies feature-wise\n",
    "print(\"ud\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(teddi_10k, 0.1)) #entropies feature-wise\n",
    "print(\"teddi\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(mbert_10k, 0.1)) #entropies feature-wise\n",
    "print(\"mbert\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(bibles_10k, 0.1)) #entropies feature-wise\n",
    "print(\"bibles\", mean(entropies_features)) #mean of entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "naughty-preserve",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bin Size: 0.9\n",
      "features: 12\n"
     ]
    }
   ],
   "source": [
    "print(\"Bin Size: 0.9\")\n",
    "\n",
    "print(\"features:\", str(len(np.arange(1, 11.2, 0.9))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "damaged-diesel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xcopa 0.27950363755876206\n",
      "xquad 0.3017539109227398\n",
      "tydiqa 0.30776927247070457\n",
      "xnli 0.29401904932295897\n",
      "xtreme 0.3063741359892156\n",
      "xglue 0.25649337673292716\n",
      "ud 0.3187610112633042\n",
      "teddi 0.33328568075800086\n",
      "mbert 0.3006660186898331\n",
      "bibles 0.30494797843982663\n"
     ]
    }
   ],
   "source": [
    "entropies_features=get_entropy(get_wordlength_vectors2(xcopa_10k, 0.9)) #entropies feature-wise\n",
    "print(\"xcopa\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(xquad_10k, 0.9)) #entropies feature-wise\n",
    "print(\"xquad\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(tydiqa_10k, 0.9)) #entropies feature-wise\n",
    "print(\"tydiqa\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(xnli_10k, 0.9)) #entropies feature-wise\n",
    "print(\"xnli\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(xtreme_10k, 0.9)) #entropies feature-wise\n",
    "print(\"xtreme\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(xglue_10k, 0.9)) #entropies feature-wise\n",
    "print(\"xglue\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(ud_10k, 0.9)) #entropies feature-wise\n",
    "print(\"ud\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(teddi_10k, 0.9)) #entropies feature-wise\n",
    "print(\"teddi\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(mbert_10k, 0.9)) #entropies feature-wise\n",
    "print(\"mbert\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(bibles_10k, 0.9)) #entropies feature-wise\n",
    "print(\"bibles\", mean(entropies_features)) #mean of entropies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "becoming-israeli",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bin Size: 0.8\n",
      "features: 13\n"
     ]
    }
   ],
   "source": [
    "print(\"Bin Size: 0.8\")\n",
    "\n",
    "print(\"features:\", str(len(np.arange(1, 11.2, 0.8))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "auburn-ivory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xcopa 0.29049693979866065\n",
      "xquad 0.2942449739504756\n",
      "tydiqa 0.30549351966050564\n",
      "xnli 0.27140219937503907\n",
      "xtreme 0.28044117924825046\n",
      "xglue 0.2776086630367018\n",
      "ud 0.30436492366351614\n",
      "teddi 0.3245412407719476\n",
      "mbert 0.29564414532318517\n",
      "bibles 0.28887418064142617\n"
     ]
    }
   ],
   "source": [
    "entropies_features=get_entropy(get_wordlength_vectors2(xcopa_10k, 0.8)) #entropies feature-wise\n",
    "print(\"xcopa\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(xquad_10k, 0.8)) #entropies feature-wise\n",
    "print(\"xquad\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(tydiqa_10k, 0.8)) #entropies feature-wise\n",
    "print(\"tydiqa\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(xnli_10k, 0.8)) #entropies feature-wise\n",
    "print(\"xnli\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(xtreme_10k, 0.8)) #entropies feature-wise\n",
    "print(\"xtreme\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(xglue_10k, 0.8)) #entropies feature-wise\n",
    "print(\"xglue\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(ud_10k, 0.8)) #entropies feature-wise\n",
    "print(\"ud\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(teddi_10k, 0.8)) #entropies feature-wise\n",
    "print(\"teddi\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(mbert_10k, 0.8)) #entropies feature-wise\n",
    "print(\"mbert\", mean(entropies_features)) #mean of entropies\n",
    "\n",
    "entropies_features=get_entropy(get_wordlength_vectors2(bibles_10k, 0.8)) #entropies feature-wise\n",
    "print(\"bibles\", mean(entropies_features)) #mean of entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-package",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-student",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
